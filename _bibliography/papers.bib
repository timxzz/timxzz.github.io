---
---

@string{aps = {American Physical Society,}}

@InProceedings{liu2025fast,
  abbr={ICLR},
  title={Fast Diversity-Preserving Reward Finetuning of Diffusion Models via Nabla-GFlowNets},
  author={Liu, Zhen and Xiao*, Tim Z. and Liu*, Weiyang and Bengio, Yoshua and Zhang, Dinghuai},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  bibtex_show={true},
  html={https://nabla-gfn.github.io/},
  arxiv={2412.07775},
  pdf={https://arxiv.org/pdf/2412.07775},
}

@InProceedings{qiu2025can,
  abbr={ICLR},
  title={Can Large Language Models Understand Symbolic Graphics Programs?},
  author={Qiu*, Zeju and Liu*, Weiyang and Feng*, Haiwen and Liu, Zhen and Xiao, Tim Z. and Collins, Katherine M. and Tenenbaum, Joshua B. and Weller, Adrian and Black, Michael J. and Schölkopf, Bernhard},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  bibtex_show={true},
  html={https://openreview.net/forum?id=Yk87CwhBDx},
  arxiv={2408.08313},
  pdf={https://arxiv.org/pdf/2408.08313},
}

@InProceedings{ou2025improving,
  abbr={ICLR},
  title={Improving Probabilistic Diffusion Models With Optimal Covariance Matching},
  author={Ou*, Zijing and Zhang*, Mingtian and Zhang, Andi and Xiao, Tim Z. and Li, Yingzhen and Barber, David},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  bibtex_show={true},
  html={https://openreview.net/forum?id=fV0t65OBUu},
  arxiv={2406.10808},
  pdf={https://arxiv.org/pdf/2406.10808},
}

@InProceedings{zhang2025your,
  abbr={AISTATS},
  title={Your Finetuned Large Language Model is Already a Powerful Out-of-distribution Detector},
  author={Zhang, Andi and Xiao, Tim Z. and Liu, Weiyang and Bamler, Robert and Wischik, Damon},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2025},
  bibtex_show={true},
  arxiv={2404.08679},
  pdf={https://arxiv.org/pdf/2404.08679},
}

@InProceedings{xiao2025verbalized,
    abbr={TMLR},
    title={Verbalized Machine Learning: Revisiting Machine Learning with Language Models},
    author={Xiao, Tim Z. and Bamler, Robert and Schölkopf, Bernhard and Liu, Weiyang},
    booktitle={Transactions on Machine Learning Research (TMLR)},
    bibtex_show={true},
    code={https://github.com/timxzz/VML_Examples},
    html={https://openreview.net/forum?id=k3Ab6RuJE9},
    year={2025},
    pdf={vml_v1.pdf},
    arxiv={2406.04344},
    preview={vml_preview.png},
    selected={true}
  }

@InProceedings{xiao2024a,
  abbr={TMLR},
  title={A Note on Generalization in Variational Autoencoders: How Effective Is Synthetic Data and Overparameterization?},
  author={Xiao*, Tim Z. and Zenn*, Johannes and Bamler, Robert},
  booktitle={Transactions on Machine Learning Research (TMLR)},
  year={2024},
  arxiv={2310.19653},
  bibtex_show={true},
  html={https://openreview.net/forum?id=bwyHf5eery},
  pdf={DMaaPx.pdf},
  preview={dmaapx.png},
  selected={true}
}

@article{xiao2023compact,
    abbr={Workshop},
    title={A Compact Representation for Bayesian Neural Networks By Removing Permutation Symmetry},
    author={Xiao, Tim Z. and Liu, Weiyang and Bamler, Robert},
    bibtex_show={true},
    journal={arXiv preprint arXiv:2401.00611},
    workshop={NeurIPS 2023 Workshop on Unifying Representations in Neural Models},
    year={2023},
    html={https://unireps.org},
    pdf={UniReps_ABI_Rebasin_v2.pdf},
    poster={UniReps_ABI_Rebasin_Poster_ps.pdf},
    arxiv={2401.00611},
    preview={unireps_rebasin.png},
    selected={true}
  }

@article{xiao2023the,
    abbr={Workshop},
    title={The SVHN Dataset Is Deceptive for Probabilistic Generative Models Due to a Distribution Mismatch},
    author={Xiao*, Tim Z. and Zenn*, Johannes and Bamler, Robert},
    bibtex_show={true},
    journal={arXiv preprint arXiv:2312.02168},
    workshop={NeurIPS 2023 Workshop on Distribution Shifts},
    year={2023},
    html={https://jzenn.github.io/svhn-remix},
    pdf={SVHN_Remix_v1.pdf},
    poster={SVHN_Remix_Poster_ps.pdf},
    arxiv={2312.02168},
    preview={svhn_remix.png},
    selected={true}
  }

@InProceedings{xiao2023trading,
  abbr={ICLR},
  title={Trading Information between Latents in Hierarchical Variational Autoencoders},
  author={Xiao, Tim Z. and Bamler, Robert},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023},
  arxiv={2302.04855},
  bibtex_show={true},
  code={https://github.com/timxzz/HIT},
  html={https://openreview.net/forum?id=eWtMdr6yCmL},
  pdf={HIT.pdf},
  poster={HIT_poster.pdf},
  slides={HIT_slides.pdf},
  preview={hit_3d.png},
  selected={true}
}

@InProceedings{Qiu2023DHT,
  abbr={AISTATS},
  title={Iterative Teaching by Data Hallucination},
  author={Qiu*, Zeju and Liu*, Weiyang and Xiao, Tim Z. and Liu, Zhen 
    and Bhatt, Umang and Luo, Yucen and Weller, Adrian and Schölkopf, Bernhard},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2023},
  bibtex_show={true},
  arxiv={2210.17467},
  code={https://github.com/Zeju1997/data_halucination_teaching},
  pdf={DHT.pdf},
}

@article{zhang2022out,
  abbr={Workshop},
  title={Out-of-Distribution Detection with Class Ratio Estimation},
  author={Zhang, Mingtian and Zhang, Andi and Xiao, Tim Z. and Sun, Yitong and McDonagh, Steven},
  journal={arXiv preprint arXiv:2206.03955},
  year={2022},
  bibtex_show={true},
  arxiv={2206.03955},
  workshop={NeurIPS 2022 Workshop on Machine Learning Safety},
  html={https://neurips2022.mlsafety.org}
}


@article{zhang2022improving,
  abbr={arXiv},
  title={Improving VAE-Based Representation Learning},
  author={Zhang, Mingtian and Xiao, Tim Z. and Paige, Brooks and Barber, David},
  journal={arXiv preprint arXiv:2205.14539},
  year={2022},
  bibtex_show={true},
  arxiv={2205.14539},
}

@article{shah2021locally,
  abbr={arXiv},
  title={Locally-Contextual Nonlinear CRFs for Sequence Labeling},
  author={Shah, Harshil and Xiao, Tim Z. and Barber, David},
  journal={arXiv preprint arXiv:2103.16210},
  year={2021},
  bibtex_show={true},
  arxiv={2103.16210},
}

@mastersthesis{xiao2021exploiting,
    author = {Xiao, Tim Z.},
    school = {University College London},
    title = {Exploiting Semi-Supervised Generative Model in Active Learning},
    year={2021},
    bibtex_show={true},
    preview={ucl_logo.png},
    workshop={Master's Thesis at UCL},
    pdf={AL_Thesis.pdf},
    abstract={My master project at UCL. 
              Active learning tries to solve a practical problem for machine learning, 
              which is to create a good model with only limited labelling budget. 
              In this project, we exploit useful properties from semi-supervised generative model 
              and use them in active learning. Our experiments in the half-moon and MNIST dataset 
              show that by using semi-supervised generative model with simple acquisition function 
              such as predictive entropy, we are able to improve the performance of active learning. 
              Further experiments on our proposed acquisition functions expose interesting challenges 
              in using data density provided by the model, which can be a valuable pointer for future 
              active learning research.}
}

@article{lyu2020you,
  title={You Need Only Uncertain Answers: Data Efficient Multilingual Question Answering},
  author={Lyu, Zhihao and Duolikun, Danier and Dai, Bowei and Yao, Yuan and Minervini, Pasquale and Xiao, Tim Z. and Gal, Yarin},
  journal={ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning},
  year={2020},
  bibtex_show={true},
  preview={uncertainty-qa.png},
  html={https://sites.google.com/view/udlworkshop2020/accepted-papers},
  pdf={uncertainty_qa.pdf},
  abstract={I am an advisor for this project. 
            Data scarcity is a major barrier for multilingual question answering: 
            current systems work well with languages such as English where data is affluent, 
            but face challenges with small corpora. As data labelling is expensive, previous 
            works have resorted to pre-tuning systems on larger multilingual corpora followed 
            by fine-tuning on the smaller ones. Instead of curating and labelling large corpora, 
            we demonstrate a data efficient multi-lingual question answering system which only 
            selects uncertain questions for labelling, reducing labelling efforts and costs. 
            To realise this Bayesian active learning framework, we develop methodology to quantify 
            uncertainty in several state-of-art attention-based Transfer question answering models. 
            We then propose an uncertainty measure based on the variance of BLEU scores, and computed 
            via Monte Carlo Dropout, to detect out-of-distribution questions. We finish by showing 
            the effectiveness of our uncertainty measures in various out-of-distribution question 
            answering settings.}
}

@article{xiao2020wat,
  title={Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers},
  author={Xiao, Tim Z. and Gomez, Aidan N. and Gal, Yarin},
  journal={arXiv preprint arXiv:2006.08344},
  year={2020},
  selected={true},
  bibtex_show={true},
  arxiv={2006.08344},
  preview={uncertainty-transformer.png},
  pdf={uncertainty_transformer_arxiv.pdf},
  poster={uncertainty_transformer_poster.pdf},
  slides={uncertainty_transformer_slides.pdf},
  highlight={Spotlight talk},
  workshop={NeurIPS 2019 Workshop on Bayesian Deep Learning},
  html={http://bayesiandeeplearning.org/2019/index.html},
  abstract={My master project at Oxford. 
            In the project, we detect out-of-training-distribution sentences in Neural Machine Translation 
            using the Bayesian Deep Learning equivalent of Transformer models. For this we develop a new 
            measure of uncertainty designed specifically for long sequences of discrete random variables 
            -- i.e. words in the output sentence. Our new measure of uncertainty solves a major intractability 
            in the naive application of existing approaches on long sentences. We use our new measure on a 
            Transformer model trained with dropout approximate inference. On the task of German-English 
            translation using WMT13 and Europarl, we show that with dropout uncertainty our measure is able 
            to identify when Dutch source sentences, sentences which use the same word types as German, 
            are given to the model instead of German.}
}

@InProceedings{xiao2016fpga,
  author={Zhenzhong Xiao and Koch, Dirk and Lujan, Mikel},
  booktitle={International Conference on Field Programmable Logic and Applications (FPL)}, 
  title={A partial reconfiguration controller for Altera Stratix V FPGAs}, 
  year={2016},
  doi={10.1109/FPL.2016.7577349},
  bibtex_show={true}
}