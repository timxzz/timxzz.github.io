@article{graves2014neural,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

@misc{akyurek2022gpt3addition,
    author = {Akyurek, Ekin and Akyurek, Afra Feyza},
    title  = {Notes on Teaching GPT-3 Adding Numbers},
    url    = {https://lingo.csail.mit.edu//blog/arithmetic_gpt3},
    year   = {2022}
}

@inproceedings{khot2022hey,
  title={Hey AI, Can You Solve Complex Tasks by Talking to Agents?},
  author={Khot, Tushar and Richardson, Kyle and Khashabi, Daniel and Sabharwal, Ashish},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={1808--1823},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@inproceedings{sumers2022talk,
  title={How to talk so AI will learn: Instructions, descriptions, and autonomy},
  author={Sumers, Theodore and Hawkins, Robert D and Ho, Mark K and Griffiths, Thomas L and Hadfield-Menell, Dylan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}


@article{liu2021pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}


@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}


@article{bengio2000neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={13},
  year={2000}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chatgpt2022,
 author  = {John Schulman and Barret Zoph and Christina Kim and Jacob Hilton and others},
 year={2022},
 date    = {2022-11-30},
 title   = {ChatGPT: Optimizing Language Models for Dialogue},
 journal = {OpenAI},
 url     = {https://openai.com/blog/chatgpt/},
 urldate = {2023-01-16}
}


@inproceedings{ganguli2022predictability,
  title={Predictability and surprise in large generative models},
  author={Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and others},
  booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1747--1764},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and others},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}


@article{chowdhery2022palm,
title={PaLM: Scaling Language Modeling with Pathways},
author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and others},
year={2022},
journal={arXiv preprint arxiv:2204.02311}
}



@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities.},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@article{anderson1972more,
author = {Philip W. Anderson},
title = {More Is Different},
journal = {Science},
volume = {177},
number = {4047},
pages = {393-396},
year = {1972},
doi = {10.1126/science.177.4047.393}}

@article{turing1936computable,
  title={On computable numbers, with an application to the Entscheidungsproblem},
  author={Turing, Alan},
  journal={J. of Math},
  volume={58},
  number={345-363},
  pages={5},
  year={1936}
}

@article{weik1961eniac,
  title={The ENIAC story},
  author={Weik, Martin H},
  journal={Ordnance},
  volume={45},
  number={244},
  pages={571--575},
  year={1961},
  publisher={JSTOR}
}


@article{von1945first,
  title={First Draft of a Report on the EDVAC},
  author={Von Neumann, John},
  journal={Pennsylvania: University of Pennsylvania},
  year={1945}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{kaiser2017one,
  title={One model to learn them all},
  author={Kaiser, Lukasz and Gomez, Aidan N and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1706.05137},
  year={2017}
}



@inproceedings{shazeer2017outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{mccann2018natural,
  title={The natural language decathlon: Multitask learning as question answering},
  author={McCann, Bryan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1806.08730},
  year={2018}
}


@inproceedings{dehghani2018universal,
  title={Universal Transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{zhou2022teaching,
  title={Teaching Algorithmic Reasoning via In-context Learning},
  author={Zhou, Hattie and Nova, Azade and Larochelle, Hugo and Courville, Aaron and Neyshabur, Behnam and Sedghi, Hanie},
  journal={arXiv preprint arXiv:2211.09066},
  year={2022}
}


@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and others},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}